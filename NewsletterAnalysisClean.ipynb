{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "filenameMembers = \"115Congress.csv\"\n",
    "names = pd.read_csv(filenameMembers,encoding='utf-8')\n",
    "firstNames = []\n",
    "lastNames = []\n",
    "fileNameState = \"stateDecoder.csv\"\n",
    "stateCode = pd.read_csv(fileNameState,encoding='utf-8')\n",
    "fileNamePD = \"partisanDistricts.csv\"\n",
    "partisanDistricts = pd.read_csv(fileNamePD,encoding='latin_1')\n",
    "fileNamePS = \"partisanStates.csv\"\n",
    "partisanStates = pd.read_csv(fileNamePS,encoding='latin_1')\n",
    "values = []\n",
    "#Get info about what district each member is from and what their Cook PVI is\n",
    "for _, row in names.iterrows():\n",
    "    name = row[\"Name\"]\n",
    "    parts = name.split(\", \")\n",
    "    lastName = parts[0]\n",
    "    firstName = parts[1].split(\" \")[0]\n",
    "    initial = lastName[0]\n",
    "    fullState = row[\"State Label\"]\n",
    "    potentialState = stateCode[stateCode[\"State\"] == fullState][\"Code\"].values\n",
    "    if len(potentialState)>0:\n",
    "        stateID = potentialState[0]\n",
    "    else:\n",
    "        continue\n",
    "    district = int(row[\"District\"])\n",
    "    \n",
    "    if district == 0:\n",
    "        if row[\"Chamber\"] == \"Representative\":\n",
    "            distID = \"AL\"\n",
    "        else:\n",
    "            distID = \"Sen\"\n",
    "    elif district < 10:\n",
    "        distID = \"0\"+str(district)\n",
    "    else:\n",
    "        distID = str(district)\n",
    "    \n",
    "    if distID == \"Sen\" or stateID == \"DC\":\n",
    "        pvi = partisanStates[partisanStates[\"State\"]==fullState][\"PVI\"].values[0]\n",
    "    else:\n",
    "        pvi = partisanDistricts[partisanDistricts[\"Dist\"]==stateID+\"-\"+distID][\"PVI\"].values[0]\n",
    "    if pvi[0]==\"R\":\n",
    "        convPVI = int(pvi[2:])\n",
    "    elif pvi[0] == \"D\":\n",
    "        convPVI = -int(pvi[2:])\n",
    "    else:\n",
    "        convPVI = 0\n",
    "    value = [lastName, firstName, initial, stateID, distID, convPVI]\n",
    "    values.append(value)\n",
    "newNames = pd.DataFrame(values, columns=[\"lastName\",\"firstName\",\"lastInitial\",\"State\",\"District\",\"PVI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the newsletter data\n",
    "fileNames = [\"115Congress_newsletter/02_2017.csv\",\n",
    "             \"115Congress_newsletter/03_2017.csv\",\n",
    "             \"115Congress_newsletter/04_2017.csv\",\n",
    "             \"115Congress_newsletter/05_2017.csv\",\n",
    "             \"115Congress_newsletter/06_2017.csv\",\n",
    "             \"115Congress_newsletter/07_2017.csv\",\n",
    "             \"115Congress_newsletter/08_2017.csv\",\n",
    "             \"115Congress_newsletter/09_2017.csv\",\n",
    "             \"115Congress_newsletter/10_2017.csv\",\n",
    "             \"115Congress_newsletter/11_2017.csv\",\n",
    "             \"115Congress_newsletter/12_2017.csv\",\n",
    "             \"115Congress_newsletter/01_2018.csv\",\n",
    "             \"115Congress_newsletter/02_2018.csv\",\n",
    "             \"115Congress_newsletter/03_2018.csv\",\n",
    "             \"115Congress_newsletter/04_2018.csv\",\n",
    "             \"115Congress_newsletter/05_2018.csv\",\n",
    "             \"115Congress_newsletter/06_2018.csv\",\n",
    "             \"115Congress_newsletter/07_2018.csv\",\n",
    "             \"115Congress_newsletter/08_2018.csv\",\n",
    "             \"115Congress_newsletter/09_2018.csv\",\n",
    "             \"115Congress_newsletter/10_2018.csv\",\n",
    "             \"115Congress_newsletter/11_2018.csv\",\n",
    "             \"115Congress_newsletter/12_2018.csv\"\n",
    "            ]\n",
    "data = pd.read_csv(fileNames[0])\n",
    "\n",
    "data[\"Date\"] = date(int(fileNames[0][-8:-4]),int(fileNames[0][-11:-9]),1)\n",
    "for fileName in fileNames[1:]:\n",
    "    temp = pd.read_csv(fileName)\n",
    "    temp[\"Date\"] = date(int(fileName[-8:-4]),int(fileName[-11:-9]),1)\n",
    "    data = data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/24092 emails processed\n",
      "400/24092 emails processed\n",
      "800/24092 emails processed\n",
      "1200/24092 emails processed\n",
      "1600/24092 emails processed\n",
      "2000/24092 emails processed\n",
      "2400/24092 emails processed\n",
      "2800/24092 emails processed\n",
      "3200/24092 emails processed\n",
      "3600/24092 emails processed\n",
      "4000/24092 emails processed\n",
      "4400/24092 emails processed\n",
      "4800/24092 emails processed\n",
      "5200/24092 emails processed\n",
      "5600/24092 emails processed\n",
      "6000/24092 emails processed\n",
      "6400/24092 emails processed\n",
      "6800/24092 emails processed\n",
      "7200/24092 emails processed\n",
      "7600/24092 emails processed\n",
      "8000/24092 emails processed\n",
      "8400/24092 emails processed\n",
      "8800/24092 emails processed\n",
      "9200/24092 emails processed\n",
      "9600/24092 emails processed\n",
      "10000/24092 emails processed\n",
      "10400/24092 emails processed\n",
      "10800/24092 emails processed\n",
      "11200/24092 emails processed\n",
      "11600/24092 emails processed\n",
      "12000/24092 emails processed\n",
      "12400/24092 emails processed\n",
      "12800/24092 emails processed\n",
      "13200/24092 emails processed\n",
      "13600/24092 emails processed\n",
      "14000/24092 emails processed\n",
      "14400/24092 emails processed\n",
      "14800/24092 emails processed\n",
      "15200/24092 emails processed\n",
      "15600/24092 emails processed\n",
      "16000/24092 emails processed\n",
      "16400/24092 emails processed\n",
      "16800/24092 emails processed\n",
      "17200/24092 emails processed\n",
      "17600/24092 emails processed\n",
      "18000/24092 emails processed\n",
      "18400/24092 emails processed\n",
      "18800/24092 emails processed\n",
      "19200/24092 emails processed\n",
      "19600/24092 emails processed\n",
      "20000/24092 emails processed\n",
      "20400/24092 emails processed\n",
      "20800/24092 emails processed\n",
      "21200/24092 emails processed\n",
      "21600/24092 emails processed\n",
      "22000/24092 emails processed\n",
      "22400/24092 emails processed\n",
      "22800/24092 emails processed\n",
      "23200/24092 emails processed\n",
      "23600/24092 emails processed\n",
      "24000/24092 emails processed\n",
      "Fraction of messages that we successfully identified author of:  0.879041965879374\n"
     ]
    }
   ],
   "source": [
    "#These newsletters aren't lebelled with who sent them, so this performs a search to discover that\n",
    "def findAuth(subj, text,init):\n",
    "    if type(text) != str:\n",
    "        return \"idk\"\n",
    "    possAuths = newNames[newNames[\"lastInitial\"]==init][\"lastName\"].values\n",
    "    poss = []\n",
    "    #First search their last name in the subject and then body. We know their last name has to \n",
    "    #Start with the first letter of the bioguide_id \n",
    "    for auth in possAuths:\n",
    "        search = re.compile(r\"\"\"(\\s|,|\\.)(%s).?(\\s|,|\\.|\\))\"\"\" % auth)\n",
    "        occurrences = search.subn(\"\", subj)[1]\n",
    "        if occurrences > 0:\n",
    "            poss.append(auth)\n",
    "            continue\n",
    "        search = re.compile(r\"\"\"(\\s|,|\\.)(%s).?(\\s|,|\\.|\\))\"\"\" % auth)\n",
    "        occurrences = search.subn(\"\", text)[1]\n",
    "        if occurrences > 0:\n",
    "            poss.append(auth)\n",
    "            continue\n",
    "        search = re.compile(r\"\"\"(%s).?\"\"\" % auth)\n",
    "        occurrences = search.subn(\"\", text)[1]\n",
    "        if occurrences > 0:\n",
    "            poss.append(auth)\n",
    "            continue\n",
    "        search = re.compile(r\"\"\"(%s).?\"\"\" % auth.lower())\n",
    "        occurrences = search.subn(\"\", text)[1]\n",
    "        if occurrences > 0:\n",
    "            poss.append(auth)\n",
    "            continue\n",
    "    #If there are too many potential matches, look for state references in the text\n",
    "    if len(poss) > 1:\n",
    "        states = newNames[newNames[\"lastName\"].isin(poss)][\"State\"].values\n",
    "        names = newNames[newNames[\"lastName\"].isin(poss)][\"lastName\"].values\n",
    "        possStates = []\n",
    "        newNamePoss = []\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            name = names[i]\n",
    "            search = re.compile(r\"\"\"(\\s|,|.)(%s).?(\\s|,|\\.|\\))\"\"\" % state)\n",
    "            occurrences = search.subn(\"\", text)[1]\n",
    "            if occurrences > 0:\n",
    "                newNamePoss.append(name)\n",
    "                possStates.append(state)\n",
    "                continue\n",
    "            search = re.compile(r\"\"\"(\\s|,|.)(%s).?(\\s|,|\\.|\\))\"\"\" % stateCode[stateCode[\"Code\"]==state][\"State\"].values[0])\n",
    "            occurrences = search.subn(\"\", text)[1]\n",
    "            if occurrences > 0:\n",
    "                possStates.append(state)\n",
    "                newNamePoss.append(name)\n",
    "                continue\n",
    "        if len(possStates) == 1:\n",
    "            return (newNames[(newNames[['lastName','State']].values == [newNamePoss[0], possStates[0]]).all(1)])\n",
    "        #If too many matches still, look for first name references\n",
    "        else:\n",
    "            firsts = newNames[newNames[\"lastName\"].isin(poss)][\"firstName\"].values\n",
    "            names = newNames[newNames[\"lastName\"].isin(poss)][\"lastName\"].values\n",
    "            possFirsts = []\n",
    "            newNamePoss = []\n",
    "            for i in range(len(firsts)):\n",
    "                fn = firsts[i]\n",
    "                name = names[i]\n",
    "                search = re.compile(r\"\"\"(\\s|,|.)(%s).?(\\s|,|\\.|\\))\"\"\" % fn)\n",
    "                occurrences = search.subn(\"\", text)[1]\n",
    "                if occurrences > 0:\n",
    "                    newNamePoss.append(name)\n",
    "                    possFirsts.append(fn)\n",
    "                    continue\n",
    "            if len(possFirsts) == 1:\n",
    "                return (newNames[(newNames[['lastName','firstName']].values == [newNamePoss[0], possFirsts[0]]).all(1)])\n",
    "            else:\n",
    "                return (poss,possStates,newNamePoss)\n",
    "    #If no reference to the last name, look for state reference and hope that there's only one congressperson from that state with a \n",
    "    #last name starting with the letter of interest\n",
    "    elif len(poss) == 0:\n",
    "        states = newNames[newNames[\"lastName\"].isin(possAuths)][\"State\"].values\n",
    "        names = newNames[newNames[\"lastName\"].isin(possAuths)][\"lastName\"].values\n",
    "        possStates = []\n",
    "        newNamePoss = []\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            name = names[i]\n",
    "            search = re.compile(r\"\"\"(\\s|,|.)(%s).?(\\s|,|\\.|\\))\"\"\" % state)\n",
    "            occurrences = search.subn(\"\", text)[1]\n",
    "            if occurrences > 0:\n",
    "                newNamePoss.append(name)\n",
    "                possStates.append(state)\n",
    "                continue\n",
    "            search = re.compile(r\"\"\"(\\s|,|.)(%s).?(\\s|,|\\.|\\))\"\"\" % stateCode[stateCode[\"Code\"]==state][\"State\"].values[0])\n",
    "            occurrences = search.subn(\"\", text)[1]\n",
    "            if occurrences > 0:\n",
    "                possStates.append(state)\n",
    "                newNamePoss.append(name)\n",
    "                continue\n",
    "        if len(possStates) == 1:\n",
    "            return (newNames[(newNames[['lastName','State']].values == [newNamePoss[0], possStates[0]]).all(1)])\n",
    "        #Look for first name reference--sometimes congresspeople use just first names to seem more approachable\n",
    "        else:\n",
    "            firsts = newNames[newNames[\"lastName\"].isin(possAuths)][\"firstName\"].values\n",
    "            names = newNames[newNames[\"lastName\"].isin(possAuths)][\"lastName\"].values\n",
    "            possFirsts = []\n",
    "            newNamePoss = []\n",
    "            for i in range(len(firsts)):\n",
    "                fn = firsts[i]\n",
    "                name = names[i]\n",
    "                search = re.compile(r\"\"\"(\\s|,|.)(%s).?(\\s|,|\\.|\\))\"\"\" % fn)\n",
    "                occurrences = search.subn(\"\", text)[1]\n",
    "                if occurrences > 0:\n",
    "                    newNamePoss.append(name)\n",
    "                    possFirsts.append(fn)\n",
    "                    continue\n",
    "            if len(possFirsts) == 1:\n",
    "                return (newNames[(newNames[['lastName','firstName']].values == [newNamePoss[0], possFirsts[0]]).all(1)])\n",
    "        return (\"idk\")\n",
    "    else:\n",
    "        return (newNames[newNames[\"lastName\"] == poss[0]])\n",
    "bad = 0\n",
    "good = 0\n",
    "globData = []\n",
    "totlen = len(data)\n",
    "for testval in range(len(data)):\n",
    "    if testval%400 == 0:\n",
    "        print(str(testval)+\"/\"+str(totlen)+\" emails processed\")\n",
    "    if type(data[\"bioguide_id\"].values[testval])!= str:\n",
    "        continue\n",
    "    trueAuth = findAuth(data[\"Subject\"].values[testval],data[\"Message\"].values[testval],data[\"bioguide_id\"].values[testval][0])\n",
    "    if type(trueAuth) != pd.DataFrame:\n",
    "        bad += 1\n",
    "    elif len(trueAuth) > 1:\n",
    "        bad += 1\n",
    "    else:\n",
    "        good += 1\n",
    "        #Made a dataframe with the messages whose author we could identify\n",
    "        globData.append([data['Subject'].values[testval],data['Message'].values[testval],data[\"Date\"].values[testval],\n",
    "                         trueAuth[\"lastName\"].values[0],trueAuth[\"firstName\"].values[0],\n",
    "              trueAuth[\"State\"].values[0], trueAuth[\"District\"].values[0],trueAuth[\"PVI\"].values[0]])\n",
    "globData = pd.DataFrame(globData,columns=['Subject','Text',\"Date\",\"lastName\",\"firstName\",\"State\", \n",
    "                         \"District\",\"PVI\"])\n",
    "print(\"Fraction of messages that we successfully identified author of: \", good/(good+bad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that cuts off the beginning and end of a message. This is because these parts are usually uninformative \n",
    "#about content\n",
    "def cutEnds(text):\n",
    "    firstfrac = .06\n",
    "    lastfrac = .13\n",
    "    totlen = len(text)\n",
    "    return(text[int(firstfrac*totlen):(totlen-int(lastfrac*totlen))])\n",
    "textAll = globData[\"Text\"].values\n",
    "t2 = []\n",
    "for t in textAll:\n",
    "    t2.append(cutEnds(t))\n",
    "globData[\"TextS\"] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Democrat analysis\n",
      "Finished Republican analysis\n",
      "Finished Neutral analysis\n"
     ]
    }
   ],
   "source": [
    "partDivide = 5 #difference from average that will constitute \"partisan\" (median +/- partDivide)\n",
    "med = np.median(globData[\"PVI\"].values)\n",
    "#Democratic-leaning districts\n",
    "dems = globData[globData[\"PVI\"]<=-partDivide+med]\n",
    "demtrain, demtest = train_test_split(dems, test_size=0.1)\n",
    "#Republican leaning districts\n",
    "repubs = globData[globData[\"PVI\"]>=partDivide+med]\n",
    "reptrain, repubtest = train_test_split(repubs, test_size=0.1)\n",
    "#\"Neutral\" middle districts\n",
    "neuts = globData[(-partDivide+med<globData[\"PVI\"]) & (globData[\"PVI\"]<partDivide+med)]\n",
    "neuttrain, neuttest = train_test_split(neuts, test_size=0.1)\n",
    "\n",
    "#Words that we want to ignore because they show up unhelpfully in many emails\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(['unsubscribe', 'privacy', 'policy', 'washington', 'dc', '202', \n",
    "                                               '255', '20515', 'phone', 'click', 'open', 'fax', 'browser',  \n",
    "                                               'window', 'mail','text','version','email','plain',\n",
    "                                              'house','district','office','building','225','dear',\n",
    "                                              'friend','member','congress','congressional','look',\n",
    "                                              'forward','hob',]\n",
    ")\n",
    "\n",
    "vect = TfidfVectorizer(max_features=20, min_df=5, max_df=0.2,ngram_range=(2,5), stop_words=my_stop_words)\n",
    "\n",
    "summariesd = \"\".join(demtrain['TextS'])\n",
    "ngrams_summariesd = vect.build_analyzer()(summariesd)\n",
    "cd = Counter(ngrams_summariesd)\n",
    "print('Finished Democrat analysis')\n",
    "summariesr = \"\".join(reptrain['TextS'])\n",
    "ngrams_summariesr = vect.build_analyzer()(summariesr)\n",
    "cr = Counter(ngrams_summariesr)\n",
    "print('Finished Republican analysis')\n",
    "summariesn = \"\".join(neuttrain['TextS'])\n",
    "ngrams_summariesn = vect.build_analyzer()(summariesn)\n",
    "cn = Counter(ngrams_summariesn)\n",
    "print('Finished Neutral analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [summariesd, summariesr, summariesn]\n",
    "vect = CountVectorizer(analyzer='word', ngram_range=(2, 4),stop_words=my_stop_words)\n",
    "X = vect.fit_transform(corpus)\n",
    "X0 = np.asarray(X.todense())\n",
    "X = np.array([X0[0]/len(dems),\n",
    "    X0[1]/len(repubs),\n",
    "    X0[2]/len(neuts)])\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "df = pd.DataFrame(np.array(X).transpose(), index=feature_names, columns=[\"Dems\",\"Reps\",\"Neuts\"])\n",
    "\n",
    "df[\"DemFreq-NeutFreq\"]=df[\"Dems\"].to_numpy()-df[\"Neuts\"].to_numpy()\n",
    "df[\"RepFreq-NeutFreq\"]=df[\"Reps\"].to_numpy()-df[\"Neuts\"].to_numpy()\n",
    "df[\"RepFreq-DemFreq\"]=df[\"Reps\"].to_numpy()-df[\"Dems\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOrdered by frequency of Democratic emails mentioning phrase:\")\n",
    "print(tabulate(df.sort_values(\"Dems\",ascending=False).head(20),headers=\"keys\"))\n",
    "print(\"\\nOrdered by frequency of Republican emails mentioning phrase\")\n",
    "print(tabulate(df.sort_values(\"Reps\",ascending=False).head(20),headers=\"keys\"))\n",
    "print(\"\\nOrdered by frequency of Neutral emails mentioning phrase\")\n",
    "print(tabulate(df.sort_values(\"Neuts\",ascending=False).head(20),headers=\"keys\"))\n",
    "\n",
    "print(\"\\nOrdered by excess of frequency in Democratic emails over Neutral emails\")\n",
    "print(tabulate(df.sort_values(\"DemFreq-NeutFreq\",ascending=False).head(20),headers=\"keys\"))\n",
    "print(\"\\nOrdered by excess of frequency in Republican emails over Neutral emails\")\n",
    "print(tabulate(df.sort_values(\"RepFreq-NeutFreq\",ascending=False).head(20),headers=\"keys\"))\n",
    "print(\"\\nOrdered by excess of frequency in Republican emails over Democratic emails\")\n",
    "print(tabulate(df.sort_values(\"RepFreq-DemFreq\",ascending=False).head(20),headers=\"keys\"))\n",
    "print(\"\\nOrdered by excess of frequency in Democratic emails over Republican emails\")\n",
    "print(tabulate(df.sort_values(\"RepFreq-DemFreq\",ascending=True).head(20),headers=\"keys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfEach = 7\n",
    "\n",
    "dfrepoverdem = df.sort_values(\"RepFreq-DemFreq\",ascending=False).filter([\"Dems\",\"Reps\",\"Neuts\"]).head(numOfEach)\n",
    "dfrepoverdem.plot.bar()\n",
    "plt.title(\"Phrases Republicans prefer most over Democrats\")\n",
    "plt.ylabel(\"Fraction of emails mentioning\")\n",
    "# plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "plt.savefig(\"RepOverDem.png\",dpi=100)\n",
    "plt.show()\n",
    "\n",
    "dfdemoverrep = df.sort_values(\"RepFreq-DemFreq\",ascending=True).filter([\"Dems\",\"Reps\",\"Neuts\"]).head(numOfEach)\n",
    "dfdemoverrep.plot.bar()\n",
    "plt.title(\"Phrases Republicans prefer most over Democrats\")\n",
    "plt.ylabel(\"Fraction of emails mentioning\")\n",
    "# plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "plt.savefig(\"DemOverRep.png\",dpi=100)\n",
    "plt.show()\n",
    "\n",
    "dfrepoverneut = df.sort_values(\"RepFreq-NeutFreq\",ascending=False).filter([\"Dems\",\"Reps\",\"Neuts\"]).head(numOfEach)\n",
    "dfrepoverneut.plot.bar()\n",
    "plt.title(\"Phrases Republicans prefer most over Neutral\")\n",
    "plt.ylabel(\"Fraction of emails mentioning\")\n",
    "# plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "plt.savefig(\"RepOverNeut.png\",dpi=100)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfdemoverneut = df.sort_values(\"DemFreq-NeutFreq\",ascending=False).filter([\"Dems\",\"Reps\",\"Neuts\"]).head(numOfEach)\n",
    "dfdemoverneut.plot.bar()\n",
    "plt.title(\"Phrases Democrats prefer most over Neutral\")\n",
    "plt.ylabel(\"Fraction of emails mentioning\",dpi=100)\n",
    "# plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "plt.savefig(\"DemOverNeut.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
